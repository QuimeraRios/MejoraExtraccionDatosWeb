{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ETL librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import calendar\n",
    "import io\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from selenium import webdriver # es una librería que se utiliza para automatizar la interacción con un navegador web real\n",
    "from bs4 import BeautifulSoup #  es una librería para la extracción de datos de documentos HTML y XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Carga de archivos en un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan cada uno de los archivos de Excel en un DataFrame\n",
    "df1 = pd.read_excel('1.xlsx')\n",
    "df2 = pd.read_excel('2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener los títulos de las columnas del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los títulos de las columnas del DataFrame\n",
    "titulos_columnas1 = df1.columns\n",
    "titulos_columnas2 = df2.columns\n",
    "print(\"1: \", titulos_columnas1)\n",
    "print(\"2 :\", titulos_columnas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime los tipos de datos de cada columna\n",
    "print(df1.dtypes)\n",
    "print(df1.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificaré los valores unicos por columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datos_unicos(datafram_analisis):\n",
    "    for column in datafram_analisis.columns:\n",
    "        unique_values = datafram_analisis[column].unique()\n",
    "        respuesta= print(f\"Valores únicos por columna '{column}':\", unique_values)\n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_unicos(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###EDA\n",
    "revisando los datos podemos ver que se requieren algunas transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio de puntos a comas para los datos numericos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Valor Total']  = df1['Valor Total'] .astype(str).str.replace('.', ',')\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos las columnas numericas con un describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe() # describe las columnas numericas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis de Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValorMax= max(df1['Valor Total'])\n",
    "ValorMin=min(df1['Valor Total'])\n",
    "print(\"max: \", ValorMax, \"minimo: \", ValorMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar valores que sean menores que 'accesolimite'\n",
    "accesolimite = 100\n",
    "df_filtered = df1graf1[df1graf1['Accesos Totales'] < accesolimite]\n",
    "\n",
    "# Graficar el resultado\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x='Trimestre', y='Accesos Totales', hue='Provincia', data=df_filtered)\n",
    "#se filtra por provincia\n",
    "\n",
    "plt.title('Grafico de dispersión entre año y accesos Totales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad de duplicados\n",
    "column_names=df1.columns\n",
    "n_duplicates=df1.drop(labels=column_names,axis=1).duplicated().sum\n",
    "n_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe las columnas numericas\n",
    "df4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_unicos(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisar los valores nulos\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(df1.isna(),aspect=\"auto\", \n",
    "           interpolation=\"nearest\",cmap=\"Blues\")\n",
    "plt.xlabel(\"Columnas numericas\")\n",
    "plt.ylabel(\"Valores numericos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecciono solo las columnas numéricas\n",
    "df_numeric = df1.select_dtypes(include=['int64', 'float64'])\n",
    "# crea una matriz de gráficos de dispersión\n",
    "scatter_matrix(df_numeric, figsize=(10, 10), diagonal='kde')\n",
    "\n",
    "# muestra el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Año'] = df1['Año'].str.replace(' ', '')\n",
    "df1['Año'] = df1['Año'].str.replace('*', '')\n",
    "#df1['Año'] = df1['Año'].str.replace('*', '0')\n",
    "df1['Trimestre'] = df1['Trimestre'].str.replace('*', '')\n",
    "df1['Año'] = df1['Año'].str.strip()\n",
    "df1['Trimestre'] = df1['Trimestre'].str.strip()\n",
    "df1['Año'] = pd.to_numeric(df1['Año'], errors='coerce').fillna(0).astype(int) # Convertir valores restantes a números enteros\n",
    "df1['Trimestre'] = pd.to_numeric(df1['Trimestre'], errors='coerce').fillna(0).astype(int) # Convertir valores restantes a números enteros\n",
    "df1['Año'] = pd.to_numeric(df1['Año'], errors='coerce').fillna(0).astype(int) # Convertir valores restantes a números enteros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renombrar columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df6.rename(columns={\"Total\": \"Total_fijo_xtipo_provincia\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformar el tipo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6['ADSL'] = df6['ADSL'].astype('int64')\n",
    "df6['Cablemodem'] = df6['Cablemodem'].astype('int64')\n",
    "df6['Fibra óptica'] = df6['Fibra óptica'].astype('int64')\n",
    "df6['Wireless'] = df6['Wireless'].astype('int64')\n",
    "df6['Otros'] = df6['Otros'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.to_numeric(df11[\"Año\"], errors='coerce': Convierte la columna \"Año\" en valores numéricos..fillna(0): Reemplaza los valores NaN (que se generaron en el paso anterior) con el valor 0..astype(int): convierte todos los valores de la columna en enteros (int)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Año\"] = pd.to_numeric(df1[\"Año\"], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convertir la columna \"Año\" de un DataFrame en valores de tipo entero de 64 bits (int64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Año\"] = df1[\"Año\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista con los nombres de las columnas que contienen valores numéricos\n",
    "columnas_numericas = df1.select_dtypes(include='number').columns.tolist()\n",
    "print(columnas_numericas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones:\n",
    "Reemplazo de los caracteres -0 a 0\n",
    "convrtir a datos numeridoc\n",
    "rellenar valores Nan por 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11[columnas_Mbps] = df11[columnas_Mbps].replace('- 0', '0')\n",
    "df11['Otros'] = pd.to_numeric(df11['Otros'], errors='coerce')\n",
    "df11 = df11.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crear un gráfico de pares (pair plot) utilizando la biblioteca Seaborn en Python, se centra en las columnas numéricas de un DataFrame\n",
    "Utiliza el método select_dtypes para incluir columnas que son de tipo 'int64' o 'float64'. Esto crea un nuevo DataFrame llamado df_numeric que contiene solo las columnas numéricas de df11.\n",
    "df_numeric = df_numeric.apply(pd.to_numeric): Luego, se aplica la función pd.to_numeric a todas las columnas en el DataFrame df_numeric. Esto se hace para asegurarse de que todas las columnas numéricas se traten como números en lugar de objetos de texto. En caso de que haya algún valor no numérico en estas columnas, se convertirá en NaN.\n",
    "sns.pairplot(df_numeric, diag_kind='kde'): Aquí es donde se crea el gráfico de pares, se utiliza para mostrar la distribución de cada variable en la diagonal del gráfico en forma de estimaciones de densidad de kernel (kde), lo que proporciona una representación visual de la distribución de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df11.select_dtypes(include=['int64', 'float64'])\n",
    "df_numeric = df_numeric.apply(pd.to_numeric)\n",
    "sns.pairplot(df_numeric, diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una columna nueva que sume todos los Mbps en una sola columna\n",
    "df11.fillna(value=0, inplace=True)\n",
    "columnas_Mbps = [col for col in df11.columns if 'Mbps' in col]\n",
    "df11['Suma_Mb'] = df11[columnas_Mbps].sum(axis=1)\n",
    "df11[\"Suma_Mb\"] = df11[\"Suma_Mb\"].astype(np.int64)\n",
    "print(df11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agregar una nueva columna llamada 'Count_Suma_Mb' al DataFrame df11 y calcula el número de valores en las columnas especificadas por columnas_Mbps que son mayores que cero para cada fila en el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11['Count_Suma_Mb'] = df11.apply(lambda row: (row[columnas_Mbps] > 0).sum(), axis=1)\n",
    "print(df11['Count_Suma_Mb'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar algunas columnas no necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas que contienen 'Mbps' en su título\n",
    "columnas_eliminar = [col for col in df11.columns if 'Mbps' in col]\n",
    "df11 = df11.drop(columns=columnas_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge de los primeros dos dataframes\n",
    "merged_df = pd.merge(df1, df4, on=[\"Año\", \"Trimestre\", \"Provincia\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todas las cadenas de texto a minúsculas\n",
    "merged_df = merged_df.applymap(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "merged_df.columns = merged_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar en un archivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"datos_provincias.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad de duplicados\n",
    "column_names=merged_df.columns\n",
    "n_duplicates=merged_df.drop(labels=column_names,axis=1).duplicated().sum\n",
    "n_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graficar y ver correlaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = merged_df.select_dtypes(include=['int64', 'float64'])\n",
    "df_numeric = df_numeric.apply(pd.to_numeric)\n",
    "sns.pairplot(df_numeric, diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores faltantes por columna\n",
    "merged_df.isna().mean().sort_values().plot(\n",
    "    kind=\"bar\", figsize=(15,4),\n",
    "    title=\"Porcentaje de valores faltantes por columna\",\n",
    "    ylabel=\"Relación de valores perdidos por columna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crear una lista de nombres de columnas que contienen tipos de datos de objeto (generalmente, cadenas de texto) categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns13 = list(df13.select_dtypes(include='object').columns)\n",
    "print(string_columns13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "busco las columnas que continen una cadena de caracteres especifica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_Mbps = [col for col in df13.columns if 'mbps' in col]\n",
    "df13[columnas_Mbps] = df13[columnas_Mbps].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista con los nombres de las columnas a modificar\n",
    "columnas_mbMal = ['0,5 mbps', '0,75 mbps', '1 mbps', '1,5 mbps', '2 mbps', '3 mbps', '3,5 mbps', '4 mbps', '5 mbps', '6 mbps', '7 mbps', '8 mbps', '9 mbps', '10 mbps', '12 mbps', '15 mbps', '16 mbps', '18 mbps', '20 mbps', '24 mbps', '25 mbps', '30 mbps', '40 mbps', '50 mbps', '60 mbps', '70 mbps', '75 mbps', '100 mbps']\n",
    "\n",
    "# aplicar la función pd.to_numeric a las columnas de la lista\n",
    "df13[columnas_mbMal] = df13[columnas_mbMal].apply(pd.to_numeric, errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "busco las columnas con tipo de datos string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns13 = list(df13.select_dtypes(include='object').columns)\n",
    "print(string_columns13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borro columna que no me interesa\n",
    "df13 = df13.drop('Link Indec', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las columnas a convertir\n",
    "columnas = ['Adsl', 'Cablemodem', 'Dial up', 'Fibra optica', 'Otros', 'Satelital', 'Wimax', 'Wireless','Total general']\n",
    "\n",
    "df14[columnas] = df14[columnas].replace('- 0', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coloco la primera letra mayuscula en las columnas del dataframe ( no es practico pero asi es la instrucción)\n",
    "df14.columns = df14.columns.str.capitalize()\n",
    "# Convertir los títulos de las columnas a minúsculas\n",
    "merged_df_conectividad.columns = merged_df_conectividad.columns.str.lower()\n",
    "\n",
    "# Convertir todas las cadenas de texto a minúsculas\n",
    "merged_df_conectividad = merged_df_conectividad.applymap(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "merged_df_conectividad.columns = merged_df_conectividad.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteramos sobre las columnas y las convertimos a tipo int64\n",
    "for col in columnas:\n",
    "    \n",
    "    df14[col] = df14[col].astype('int64')\n",
    "    \n",
    "# Imprimimos el DataFrame resultante\n",
    "print(df14.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['Adsl', 'Cablemodem', 'Dialup', 'Fibraoptica', '4g', '3g','Telefoniafija','Wireless','Satelital']\n",
    "df15[columnas] = df15[columnas].replace('--', 0)\n",
    "df15[columnas] = df15[columnas].replace('SI', 1)\n",
    "df15[columnas] = df15[columnas].astype('int64')\n",
    "df16['Latitud'] = pd.to_numeric(df16['Latitud'].str.replace(',', '.').replace(r'\\.+', '.', regex=True))\n",
    "df16['Longitud'] = pd.to_numeric(df16['Longitud'].str.replace(',', '.').replace(r'\\.+', '.', regex=True))\n",
    "df16['Latitud'] = pd.to_numeric(df16['Latitud'])\n",
    "df16['Longitud'] = pd.to_numeric(df16['Longitud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecciono solo las columnas numéricas\n",
    "df_numeric = merged_df_conectividad.select_dtypes(include=['int64', 'float64'])\n",
    "# crea una matriz de gráficos de dispersión\n",
    "scatter_matrix(df_numeric, figsize=(10, 10), diagonal='kde')\n",
    "\n",
    "# muestra el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez organizadas las bases de datos se procede a continuar con el EDA en power bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_conectividad.to_csv(r'E:/DataScience/Labs/PI-2/datos_conectividad.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracción de datos desde la web en un periodo de tiempo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modificación codigo mejoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza list comprehensions: En lugar de utilizar bucles for para iterar sobre años y meses, se puede utilizar list comprehensions para generar las fechas de inicio y fin de forma más eficiente.\n",
    "\n",
    "Se Usa un solo bucle para las solicitudes HTTP: En lugar de realizar solicitudes HTTP por mes, se puede realizar una sola solicitud HTTP para cada año y luego filtrar los resultados en función de los meses. Esto reducirá la sobrecarga de las solicitudes HTTP.\n",
    "\n",
    "Usamos Parallelización: podemos utilizar la biblioteca concurrent.futures para realizar solicitudes HTTP en paralelo, lo que acelerará la descarga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para descargar datos de un rango de fechas\n",
    "def download_data(year, month):\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    starttime = f\"{year}-{month}-01\"\n",
    "    endtime = f\"{year}-{month}-{last_day}\"\n",
    "    url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={starttime}&endtime={endtime}&minmagnitude=4\"\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.text)\n",
    "    return data[\"features\"]\n",
    "\n",
    "# Lista para almacenar los datos\n",
    "all_data = []\n",
    "\n",
    "# Descargar datos de 1970 a 2022 en paralelo\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    years = range(1970, 2023)\n",
    "    months = range(1, 13)\n",
    "    futures = [executor.submit(download_data, year, month) for year in years for month in months]\n",
    "    for future in futures:\n",
    "        data = future.result()\n",
    "        for feature in data:\n",
    "            properties = feature['properties']\n",
    "            all_data.append(properties)\n",
    "\n",
    "# Descargar datos para el año 2023 (hasta el mes actual)\n",
    "mes_actual = datetime.datetime.now().month\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    months = range(1, mes_actual + 1)\n",
    "    futures = [executor.submit(download_data, 2023, month) for month in months]\n",
    "    for future in futures:\n",
    "        data = future.result()\n",
    "        for feature in data:\n",
    "            properties = feature['properties']\n",
    "            all_data.append(properties)\n",
    "\n",
    "# Crear DataFrame\n",
    "df_global = pd.DataFrame(all_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject-L3smoGf8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
